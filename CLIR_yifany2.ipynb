{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e6baea34c4f97afbf3d894accb2933e3d4bb339d51730922f3b2e344",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross-language Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "112db575ca37ca3109e62ee98e6ea483d0651d211fdad23a5e5cf75e"
   },
   "source": [
    "```Student Name: Yifan Yu\n",
    "Student ID: 702550```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "36a949c5bd75587cbc694bfd5ffd44aae070ec2c30717cd3ee3efa3d"
   },
   "source": [
    "### Warning: File encodings and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fca74611849d1e522264f033b293bdfa26b6da17eaa38795b1a2b3c2"
   },
   "source": [
    "All the text files are encoded in *utf-8*, which requires some care in using with python strings, the nltk tools and jupyter.  Please use the following method to convert strings into ASCII by escaping the special symbols. Be careful to do the conversion after tokenisation, as the escaped tags might interfere with the NLTK tokenizer and get treated as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "ee12aabafd31c027986eb9c034fd9b4f37fe938ce6eaddab29f729b1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# def tokenize(line, tokenizer=word_tokenize):\n",
    "#     utf_line = line.decode('utf-8').lower()\n",
    "#     return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "# tokenize(\"Seit damals ist er auf über 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "dc419ebe44d062a2fbb4f51083c7cf4fb06ae6af2a277ea5b0882c99"
   },
   "source": [
    "# Part 1: CLIR engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "707c4c27693395e1f46a7369f6fbf58adcd2fde4102edd39f8c8dd26"
   },
   "source": [
    "The project has two parts: first you must build a CLIR engine, comprising information retrieval and translation components, and evaluate its accuracy. Next you will propose an extension to a component of the system. In each step you will need to justify your modelling and implementation decisions, and evaluate the quality of the outputs of the system, and at the end you will need to reflect on the overall outcomes.\n",
    "\n",
    "The CLIR system will involve:\n",
    " - translating queries from German into English, the language of our text collection, using word-based translation\n",
    " - once the queries and the documents are in the same language, search over the document collection using BM25\n",
    " - evaluate the quality of ranked retreival results using the query relevance judgements\n",
    "\n",
    "Note that you could try translating the text collection into German instead, but for this project we'll stick with translating the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "77d96460be0ea94c9e5120e9b96e0104bc4f6dd21a3adfa0565495ca"
   },
   "source": [
    "Building the CLIR engine is the majority of work in the assignment, and  constitutes 70% of the assignment mark. Note that although there are several steps, they do not necessarily need to be attempted in a linear order. That is, some components are independent of others. If you're struggling with one component and cannot complete it, then you may be able to skip over it and return to it later. Where outputs are needed in a subsequent step you may want to consider ways of side-stepping this need, e.g., by using Google translate output rather than your system output for translating the queries. You should aim to answer all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map=  0.175557324841\n",
      "Run time: 0.813018136762\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def computeMap(filename):\n",
    "    eva={}\n",
    "    cidset=set()\n",
    "    for line in open(relative_path(filename)):            \n",
    "        cid, rank, sales = line.rstrip('\\n').split(\",\")\n",
    "        cidset.add(cid)\n",
    "        eva[(cid,rank)]=sales\n",
    "    l=list(cidset)\n",
    "    l.sort(key = int)\n",
    "    total=0.0\n",
    "    for cid in l:\n",
    "        i=1\n",
    "        count=0.0\n",
    "        precision=0.0\n",
    "        while i<k and ((cid,str(i)) in eva.keys()):\n",
    "            if eva[(cid,str(i))]=='1':\n",
    "                count+=1\n",
    "                precision+=count/i\n",
    "            i+=1\n",
    "        if count!=0:\n",
    "            p_ave=precision/count\n",
    "            total+=p_ave\n",
    "    map=total/len(cidset)\n",
    "    return map\n",
    "\n",
    "def relative_path(filename):\n",
    "    base_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    file_path = os.path.join( base_path +'\\\\'+ filename)\n",
    "    return file_path\n",
    "\n",
    "k=5\n",
    "start =time.clock()\n",
    "print \"map= \",computeMap('Evaluation.csv')\n",
    "end =time.clock()\n",
    "print \"Run time:\",(end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eva' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-404b535ca853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[1;34m'57'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'4'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eva' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9855c72ab4f6266494c81e2675ba60f0c6e308fb4a80548a15efbe92"
   },
   "source": [
    "## Information Retreival with BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "9db4e3b86c77dca425f2497fd69db6dcbe09e39056a4d99b9d43cee6"
   },
   "source": [
    "You should implement a vector-space retreival system using the BM25 approach. You'll need to:\n",
    " - load in the data files and tokenize the input,\n",
    " - preprocess the lexicon, e.g., by stemming and stop-word removal, \n",
    " - calculate the TF/IDF representation for all documents in the collection,\n",
    " - store an inverted index such that you can efficiently recover the documents matching a query term, and\n",
    " - implement querying in the BM25 model\n",
    " - test that it runs with some English queries (you'll have to make these up)\n",
    " \n",
    "This should run in a reasonable time over *dev.docs* (up to a few minutes to index), but beyond this does not need to be highly optimised. Feel free to use python dictionaries, sets, lists, numpy/scipy matrices etc where appropriate for best runtime, but you shouldn't need to use specialised data structures such as compression schemes or the like. Feel free to use APIs in NLTK, scikit-learn and other allowed python modules as needed (see list above.) You will probably want to save and load your index to/from disk to save repeated re-indexing every time you load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predefine and import the usual functions\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "punctuation = set(string.punctuation)\n",
    "punctuation.add('``')\n",
    "\n",
    "# use the function given for decode every inputs\n",
    "def decode(line):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    line =  utf_line.encode('ascii', 'backslashreplace') \n",
    "    return line\n",
    "\n",
    "# find the relative path of a file\n",
    "def relative_path(filename):\n",
    "    base_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    file_path = os.path.join( base_path + '\\\\data_files_ass2\\\\'+ filename)\n",
    "    return file_path\n",
    "\n",
    "#Common preprocess technique including decode, tokenize and stemming \n",
    "def preprocess_line(line):\n",
    "    words=[]\n",
    "    line=decode_encode(line)\n",
    "    for token in tokenizer(line):\n",
    "        words.append(stemmer.stem(token))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "85af2488f4c6f3bf8f6b55ab64b12b3ac564230665e1b516b6cb4c2f"
   },
   "outputs": [],
   "source": [
    "# load files and preprocess lexicon\n",
    "\n",
    "def preprocess(filename):\n",
    "    docs={}\n",
    "    doc_list={}\n",
    "    doc_id_list=[]\n",
    "    lengdoc={}\n",
    "    aveLeng=0.0\n",
    "    totallength=0.0\n",
    "    dir= relative_path(filename)\n",
    "    f=open(dir)\n",
    "    for line in f:\n",
    "        doc_id,text=line.split(\"\\t\")\n",
    "        text = decode(text)\n",
    "#       terms as the set of words for inverted index\n",
    "#       tokens as the ones stored in doc for processing\n",
    "        tokens,terms = lexicon(text)\n",
    "        docs[doc_id]= terms\n",
    "        onedoc=\" \".join(tokens)\n",
    "        doc_list[doc_id]=onedoc\n",
    "        doc_id_list.append(doc_id)\n",
    "        lengdoc[doc_id]=len(doc_list[doc_id])\n",
    "        totallength+=lengdoc[doc_id]\n",
    "    numberofdocs=len(lengdoc)\n",
    "    aveLeng=totallength/float(numberofdocs)\n",
    "#   precompute all these value for better performance in query BM25\n",
    "    return docs,lengdoc,aveLeng,numberofdocs,doc_list,doc_id_list\n",
    "\n",
    "def lexicon(doc):\n",
    "    words=[]\n",
    "    terms = set()\n",
    "    for token in doc.split():\n",
    "        if token not in stopwords: \n",
    "            term=stemmer.stem(token)\n",
    "            words.append(term)\n",
    "            terms.add(term)\n",
    "    return words,terms\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "docs,lengdoc,aveLeng,n,doc_list,doc_id_list=preprocess('dev.docs')\n",
    "# print docs\n",
    "# print lengdoc\n",
    "# print n\n",
    "# print aveLeng\n",
    "end=time.clock()\n",
    "\n",
    "print \"Run time:\",(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#inverted index,create the inverted index for easy access to the doc contain certain term\n",
    "from collections import defaultdict\n",
    "\n",
    "def getInvertedIndex(docs):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for docid, terms in docs.items():\n",
    "        for term in terms:\n",
    "            inverted_index[term].append(docid)\n",
    "    # need to keep doc lists in sorted order\n",
    "    for term, index in inverted_index.items():\n",
    "        index.sort()\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "inverted_index = getInvertedIndex(docs)\n",
    "\n",
    "end=time.clock()\n",
    "\n",
    "print \"Run time:\",(end-start)\n",
    "\n",
    "# print inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#implement querying in the BM25 model\n",
    "# test that it runs with some English queries\n",
    "\n",
    "#dummy query for testing \n",
    "\n",
    "query=\"anarch polit philosophi plural modern\"\n",
    "\n",
    "#the parameter setting in BM25 model\n",
    "k1=1.5\n",
    "k3=0\n",
    "b=0.5\n",
    "\n",
    "def calculate_bm25(n,ft,lendoc,aveLeng,fdt,fqt,k1,k3,b):\n",
    "    import math\n",
    "    idf=math.log((n-ft+0.5)/(ft + 0.5))\n",
    "    tf=float((k1 + 1) * fdt) /(k1*((1-b)+b*lendoc/aveLeng)+fdt)\n",
    "    querytf=((k3+1) *fqt) / (k3 + fqt)\n",
    "    score= idf * tf * querytf\n",
    "    return score\n",
    "\n",
    "# get the bm25 for a certain doc according to the query given\n",
    "def queryDoc(query,docs,wordsinQuery,lengdoc,invented_index):\n",
    "    score=0\n",
    "    for word in wordsinQuery:\n",
    "        fqt =get_fqt(word,query)\n",
    "        fdt =get_fdt(word,docs)\n",
    "        doc_ids=set()\n",
    "        for ids in inverted_index[word]:\n",
    "            doc_ids.add(ids)\n",
    "        ft = len(doc_ids)\n",
    "        score += calculate_bm25(n,ft,lengdoc,aveLeng,fdt,fqt,k1,k3,b)\n",
    "    return score\n",
    "\n",
    "def get_fqt(word,query):\n",
    "    fqt= query.split().count(word)\n",
    "    return fqt\n",
    "\n",
    "def get_fdt(word,doc):\n",
    "    fdt= doc.split().count(word)\n",
    "    return fdt\n",
    "\n",
    "# do one query for all the documents given and output the best 10 results with highest BM25\n",
    "def query_One (query,doc_list,inverted_index):\n",
    "    rank = {}\n",
    "    wordsinQuery=set()\n",
    "    wordsinQuery=query.split()\n",
    "    docContainQuery_id=set()\n",
    "    for words in wordsinQuery:\n",
    "        for docids in inverted_index[words]:\n",
    "            docContainQuery_id.add(docids)\n",
    "#   print docContainQuery_id\n",
    "    for id in docContainQuery_id:\n",
    "        result = queryDoc(query,doc_list[id],wordsinQuery,lengdoc[id],inverted_index)\n",
    "        rank[id] = result\n",
    "#     print rank_result_dict\n",
    "    result = sorted(rank.items(), key=lambda d:d[1], reverse=True)\n",
    "#     print result\n",
    "    return result[:10]\n",
    "\n",
    "print \"Query: \"+query\n",
    "print \"The most relevent docs according to query: \"\n",
    "print query_One (query,doc_list,inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7a31b560720da3e25a26167da4621fcd08d9898d5201aa0ad5e4ae09"
   },
   "source": [
    "In computing BM25 score, firstly, I tokenize the query into tokens, and use inverted index find the documents contain words in the query, because the documents that do not contain query have no score. Then, use the precomputed value and query information to calculate the BM25 score for all the documents. Then, Sorted the result dict and output in a descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fcdf18b1e816c90148c60565adc32597f59bddd79d33951862e2c4a8"
   },
   "source": [
    "## Translating the queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "27fe41b57d40a264fa1ca569d5594c575ea84b5e109d9cad25315564"
   },
   "source": [
    "For translation, you should implement a simple word-based translation model in a noisy channel setting, where you search for the best translation string using a language model over the English with a translation model over the back-translation of the English output string, $\\vec{e}$, into the German input string, $\\vec{f}$. This corresponds to finding the string, $\\vec{e}$ which maximises $p(\\vec{e}) p(\\vec{f} | \\vec{e})$. This has two components:\n",
    "\n",
    "### Language model\n",
    "\n",
    "The first step is to estimate a language model. You should learn both a unigram language model and a trigram language model, and compare the two. Note that the unigram will be used in the *decoding* step below. You will have to think about how to smooth your estimates, and the related problem of handling unknown words.\n",
    "\n",
    "Please use *bitext-large.en* to train your models (start with *bitext-small.en* for your development) and evaluate the perplexity of your model on *newstest2013.en*. You should justify your development decisions and parameter settings in a markdown cell, and quantify their effect on perplexity, including the differences you notice between the unigram and the trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize as tokenizer\n",
    "import os\n",
    "import math\n",
    "\n",
    "filename_bismall = \"bitext-small.en\"\n",
    "filename_bilarge = \"bitext-large.en\"\n",
    "filename_test = \"newstest2013.en\"\n",
    "\n",
    "def load(filename):\n",
    "    docs={}\n",
    "    doc_list=[]\n",
    "    docid=0\n",
    "    dir= relative_path(filename)\n",
    "    f=open(dir,'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        words=[]\n",
    "        terms = decode(line)\n",
    "        for term in tokenizer(terms):\n",
    "            words.append(term)\n",
    "        doc_list.append(words)\n",
    "    return doc_list\n",
    "\n",
    "def check_for_unk_train(word,unigram_counts):\n",
    "    if word in unigram_counts:\n",
    "        return word\n",
    "    else:\n",
    "        unigram_counts[word] = 0\n",
    "        return \"UNK\"\n",
    "\n",
    "#     put the start and end symbol to every single sentence\n",
    "def convert_sentence_train(sentence,unigram_counts):\n",
    "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s2>\"] + [\"</s1>\"]\n",
    "\n",
    "#    get the trigram, bigram, unigram counts by the single method \n",
    "def get_counts(sentences):\n",
    "    trigram_counts = defaultdict(lambda: defaultdict(dict))\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    unigram_counts = {}\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]] = trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0)+1\n",
    "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "#   as the loop stops before unigram count the end symbal, make it the same probability count with the start.\n",
    "    unigram_counts[\"</s1>\"] = unigram_counts[\"<s1>\"]\n",
    "    unigram_counts[\"</s2>\"] = unigram_counts[\"<s2>\"]\n",
    "    return unigram_counts, bigram_counts, trigram_counts, token_count\n",
    "\n",
    "#   use the log function for computing the trigram probability by using interpolation as smoothing strategy.\n",
    "def get_log_prob_interp(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count, trigram_lambda):\n",
    "    return math.log(trigram_lambda*trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i],0)/float(bigram_counts[sentence[i-2]].get(sentence[i-1],1)) + \\\n",
    "                    (1 - trigram_lambda)* trigram_lambda * bigram_counts[sentence[i-1]].get(sentence[i],0)/float(unigram_counts[sentence[i-1]]) + \\\n",
    "                    (1 - trigram_lambda)*(1-trigram_lambda) * unigram_counts[sentence[i]]/token_count)\n",
    "\n",
    "def check_for_unk_test(word,unigram_counts):\n",
    "    if word in unigram_counts and unigram_counts[word] > 0:\n",
    "        return word\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "def convert_sentence_test(sentence,unigram_counts):\n",
    "    return [\"<s1>\"] + [\"<s2>\"]+ [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s2>\"] + [\"</s1>\"]\n",
    "\n",
    "def get_sent_log_prob_interp(sentence, unigram_counts, bigram_counts,trigram_counts, token_count, trigram_lambda):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_prob_interp(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count,trigram_lambda) for i in range(2,len(sentence))])\n",
    "\n",
    "# create the training set and test set randomly if only one dataset given for held out testing.\n",
    "def separte_sets(docs):\n",
    "    from random import shuffle\n",
    "    sents = docs\n",
    "    shuffle(sents)\n",
    "    cutoff = int(0.8*len(sents))\n",
    "    training_set = sents[:cutoff]\n",
    "    test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "    return training_set,test_set\n",
    "\n",
    "training_set=[]\n",
    "test_set=[]\n",
    "\n",
    "# doc_list=load(filename_bismall)\n",
    "# doc_list=load(filename_bilarge)\n",
    "# (training_set,test_set)=separte_sets(doc_list)\n",
    "\n",
    "training_set=load(filename_bilarge)\n",
    "test_set=load(filename_test)\n",
    "\n",
    "unigrams, bigrams,trigrams, token_count = get_counts(training_set)\n",
    "\n",
    "# calculate the perplexity of the model for evaluation.\n",
    "def calculate_perplexity(sentences,unigram_counts,bigram_counts,trigram_counts, token_count, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence,unigram_counts,bigram_counts,trigram_counts,token_count, parameter)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "# several lambdas are tested in order to get the optimum value\n",
    "for trigram_lambda in [0.99,0.95,0.75,0.5,0.4,0.3,0.25,0.1,0.001]:\n",
    "    print trigram_lambda\n",
    "    print calculate_perplexity(test_set,unigrams,bigrams,trigrams,token_count,get_sent_log_prob_interp,trigram_lambda)\n",
    "\n",
    "print \"unigrams perplexity\"\n",
    "print calculate_perplexity(test_set,unigrams,bigrams,trigrams,token_count,get_sent_log_prob_interp,0)\n",
    "# print \"token_counts:\"\n",
    "# print token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram model and trigram model are trained in bitext_large.en as training set and tested on the newstest2013.en\n",
    "The result above shows that the trigram model performs better than unigram model in most cases.\n",
    "During several experiments, the best fit point of lambda in interpolation is found as 0.4, which is a low compatively. Somehow, it could be given a general idea that for higher -grams model, the parameter lambda may lose its meaning when it's too high or too low. Several other experiments were done as in the following slot:\n",
    "The unigram model and trigram model also been tested on held-out data with a 80% / 20% seperation in bitext-small and bitext-large. Some pattern are shown that unigram model perform better in the small set and trigram model perform better in the large set. It could be believed that with much larger training set, the optimum lambda for trigram would be higher and the perplexity would be smaller generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Trigram \n",
    "training set:bi-large.en\n",
    "test set:newstest2013.en\n",
    "\n",
    "0.99\n",
    "4448.41176954\n",
    "0.95\n",
    "1418.72440082\n",
    "0.75\n",
    "486.973221341\n",
    "0.5\n",
    "354.25373319\n",
    "0.4\n",
    "345.080939219\n",
    "0.3\n",
    "353.685571458\n",
    "0.25\n",
    "366.24679724\n",
    "0.1\n",
    "473.61623981\n",
    "0.001\n",
    "963.792904825\n",
    "unigrams perplexity\n",
    "1025.36245872\n",
    "\n",
    "Trigram bi-small 80/20 held out\n",
    "\n",
    "0.99\n",
    "5732.59178758\n",
    "0.95\n",
    "1402.43859919\n",
    "0.75\n",
    "371.883510992\n",
    "0.5\n",
    "241.929565207\n",
    "0.4\n",
    "228.342449343\n",
    "0.3\n",
    "227.323122287\n",
    "0.25\n",
    "232.024607199\n",
    "0.1\n",
    "285.419292972\n",
    "0.001\n",
    "537.378609594\n",
    "unigrams perplexity\n",
    "574.191464949\n",
    "\n",
    "Trigram bi-large 80/20 held out\n",
    "0.99\n",
    "1789.02446246\n",
    "0.95\n",
    "668.919718607\n",
    "0.75\n",
    "273.013977735\n",
    "0.5\n",
    "218.545393563\n",
    "0.4\n",
    "220.135843356\n",
    "0.3\n",
    "233.784980403\n",
    "0.25\n",
    "246.934505363\n",
    "0.1\n",
    "347.199256535\n",
    "0.001\n",
    "904.893302967\n",
    "unigrams perplexity\n",
    "1017.76382647\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "96690dc70e1c6808dc4279663a5967e4b8250174b13f180ac646cbc4"
   },
   "source": [
    "### Translation model\n",
    "\n",
    "The next step is to estimate translation model probabilities. For this we will use a word-based alignment model, e.g., *ibm1* to learn word-based translation probabilities using expectation maxisation. Your task is to obtain good quality word alignments from this data, which will require careful use of the word alignment models. You will want to training in both translation directions, and combining the alignments.\n",
    "\n",
    "You should use the *bitext-small* files for this purpose, and be aware that it may take a minute or two to train ibm1 on this data. Inspect some of the word alignments and translation probabilities (e.g., using a few common words in German such as *haus*) to see if your approach is working, and give some examples of output alignments that you find that are good and bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.translate import IBMModel1\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "import os\n",
    "import operator\n",
    "from nltk.tokenize import word_tokenize as tokenizer\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "stopwords_ger = set(nltk.corpus.stopwords.words('german')) \n",
    "stopwords_eng = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "filename_Eng = \"bitext-small.en\"\n",
    "filename_Ger = \"bitext-small.de\"\n",
    "\n",
    "# load the two text file and preprocess them  for further use\n",
    "def load_file(fileE,fileF):\n",
    "    text_e=[]\n",
    "    text_f=[]\n",
    "    dirE= relative_path(fileE)\n",
    "    dirF= relative_path(fileF)\n",
    "    f1=open(dirE,'r')\n",
    "    f2=open(dirF,'r')\n",
    "    lines1 = f1.readlines()\n",
    "    lines2 = f2.readlines()\n",
    "    for line in lines1:\n",
    "        line=decode(line)\n",
    "#         line = ' '.join([word for word in tokenizer(line) if word not in stopwords_eng and word not in punctuation])\n",
    "        line = ' '.join([word for word in tokenizer(line) if word not in punctuation])\n",
    "#         for token in tokenizer(line):\n",
    "#             if token not in punctuation and word not in stopwords_eng:\n",
    "#                 line = ' '.join()\n",
    "        text_e.append(line)\n",
    "#         line= removestopwords(line,'eng')\n",
    "    for line in lines2:\n",
    "        line=decode(line)\n",
    "#         line = ' '.join([word for word in tokenizer(line) if word not in stopwords_ger and word not in punctuation])\n",
    "        line = ' '.join([word for word in tokenizer(line) if word not in punctuation])\n",
    "        text_f.append(line)\n",
    "#         line= removestopwords(line,'ger')        \n",
    "    return text_e,text_f\n",
    "\n",
    "# create the bitext of combining the english and german docs in a list\n",
    "def get_bitext(list1,list2):\n",
    "    bitext = []\n",
    "    for i in range(len(list1)):\n",
    "        bitext.append((list1[i].split(),list2[i].split()))\n",
    "        #bitext.append(((list1[i].split()),(list2[i].split())))\n",
    "    return bitext\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "text_e,text_f=load_file(filename_Eng, filename_Ger)\n",
    "\n",
    "bitext_EngtoGer = get_bitext(text_e,text_f)\n",
    "bitext_GertoEng = get_bitext(text_f,text_e)\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "# bitext_EngtoGer = get_bitext(text_e[:10],text_f[:10])\n",
    "# # bitext_GertoEng = get_bitext(text_f[:5],text_e[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "299c2cc127e0355f71715f381c9f5767ff43be16cee6db687f1c56f8"
   },
   "outputs": [],
   "source": [
    "# train the translation model IBM1 from German to English and form English to German using the bitext\n",
    "# IBM1 model was trained for 10 iterations in order to get a better result\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "bt_EngtoGer=[AlignedSent(E,F) for E,F in bitext_EngtoGer]\n",
    "bt_GertoEng=[AlignedSent(E,F) for E,F in bitext_GertoEng]\n",
    "\n",
    "m_EngtoGer=IBMModel1(bt_EngtoGer, 10)\n",
    "m_GertoEng=IBMModel1(bt_GertoEng, 10)\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# m_EngtoGer.translation_table\n",
    "# bitext = []\n",
    "# bitext.append((\"green house\".split(), \"casa verde\".split()))\n",
    "# bitext.append((\"the house\".split(), \"la casa\".split()))\n",
    "\n",
    "# print bitext\n",
    "\n",
    "\n",
    "# bt = [AlignedSent(E,F) for E,F in bitext]\n",
    "# m = IBMModel1(bt, 10)\n",
    "\n",
    "# m.translation_table\n",
    "# max(m.translation_table['green'].iteritems(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print max(m_GertoEng.translation_table['haus'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "# when it come to the noun word like 'haus' which is 'house', it is easy to get the right result. \n",
    "\n",
    "print max(m_EngtoGer.translation_table['carbon'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "# the \"carbon\" is a rare seen noun but it also translate into the right meaning as \"kohlenstoff\", which could generate a idea that the direct \n",
    "# translation model of ibm1 performs pretty well in translating nouns\n",
    "\n",
    "print max(m_EngtoGer.translation_table['power'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "# The first two translation are very good, but the translation from 'power' to 'macht' is debateble as the meaning of 'power' in english veries\n",
    "# a lot, \"macht\" means the strength and power of right, which is a part of the power mean. However, when it's talking about electricity power,the\n",
    "# translation is bad. It's a problem for most of the word based translation algorithm that not able to adjust to the context.\n",
    "\n",
    "print max(m_GertoEng.translation_table['der'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "print max(m_EngtoGer.translation_table['and'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "print max(m_EngtoGer.translation_table['the'].iteritems(), key=operator.itemgetter(1))[0]\n",
    "# these are two typical bad translations, the first one is pretty wired as it should be \"the\" in English but it translated into \"detering\"\n",
    "# which doesn't make any sense at all, it could be the problem that some of the english words are embedded in the German corpus which means the \n",
    "# training set is now well enough that some of the english words are tend not to translate at all in these two cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combining the alignments \n",
    "\n",
    "#  print bitext_EngtoGer[0]\n",
    "# print bt_EngtoGer[0].alignment[0][0][0]\n",
    "# print bt_EngtoGer[0].alignment[0][0]\n",
    "# print bt_EngtoGer[0].alignment[0]\n",
    "# print bt_EngtoGer[0].alignment\n",
    "# print bt_GertoEng[0].alignment\n",
    "# print bt_EngtoGer[0]\n",
    "\n",
    "def union(bt_EngtoGer,bt_GertoEng):\n",
    "    union=list()\n",
    "    for i in range(len(bt_EngtoGer)):\n",
    "        alignments=list()\n",
    "        for align1 in bt_EngtoGer[i].alignment:\n",
    "            alignments.append(align1)\n",
    "        for align2 in bt_GertoEng[i].alignment:\n",
    "            if align2 not in alignments:\n",
    "                alignments.append(align2)\n",
    "        union.append(alignments)\n",
    "    return union\n",
    "\n",
    "def inter(bt_EngtoGer,bt_GertoEng):\n",
    "    inter=list()\n",
    "    for i in range(len(bt_EngtoGer)):\n",
    "        alignments=list()\n",
    "        alignments_inter=set()\n",
    "        for align1 in bt_EngtoGer[i].alignment:\n",
    "            alignments.append(align1)\n",
    "        for align2 in bt_GertoEng[i].alignment:\n",
    "            if align2 in alignments:\n",
    "                alignments_inter.add(align2)\n",
    "            else:\n",
    "                for align3 in alignments:\n",
    "                    if ((align2[0]==align3[1]) and (align3[0]==align2[1])):\n",
    "                        alignments_inter.add(align3)\n",
    "        inter.append(alignments_inter)\n",
    "    return inter\n",
    "\n",
    "inter_list=inter(bt_EngtoGer,bt_GertoEng)\n",
    "union_list=union(bt_EngtoGer,bt_GertoEng)\n",
    "    \n",
    "for i in range(3):\n",
    "    print \"Original bi_text:\"\n",
    "    print  bitext_EngtoGer[i]\n",
    "    print \"intersection:\"\n",
    "    print inter_list[i]\n",
    "    print \"union:\"\n",
    "    print union_list[i]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first one and the third one is pretty short which is not a good one for analyzing the alignments.\n",
    "Take the second sentence as a example.The intersection shows that 'gold' align to 'gold', of align to 'von' and ... until 'conversation' align to 'gespr\\\\xe4ch' which is a consecutive 6 words alignment, which strongly suggest that it's a possible phrase which is exactly a right phase alignment according to their meaning. Also the unions near the intersection are potential words could make a phrase alaignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9f83604fffb191c4920da4311ccc6f889a51b4e199594d02b49c968e"
   },
   "source": [
    "### Decoding \n",
    "\n",
    "Finally you'll need to solve for the best translation for a given string. For this you should do simple word-for-word translation where each word is translated independently. Use the alignments learned above (or the translation parameters) to translate each word of the queries into English. Compare taking the maximum probability of translation into English $p(e|f)$, with the noisy-channel probability, $p(f|e)p(e)$, which considers the reverse translation and a unigram language model probability. (You'll get a chance later to do something more ambitious, using the trigram model.)\n",
    "\n",
    "Use your algorithms to translate the first 100 queries into English, and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the query and preprocess\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "stopwords_ger = set(nltk.corpus.stopwords.words('german')) \n",
    "punctuation = set(string.punctuation)\n",
    "punctuation.add('``')\n",
    "filename='dev.queries'\n",
    "\n",
    "\n",
    "def preprocess(filename):\n",
    "    doc_list={}\n",
    "    dir= relative_path(filename)\n",
    "    f=open(dir,'r')\n",
    "#     the first 100 queries\n",
    "    lines = f.readlines(100)\n",
    "    num = 0\n",
    "    for line in lines:\n",
    "        num+=1\n",
    "        doc_id,text = line.split(\"\\t\")\n",
    "#       only int could be sorted in a right docs order\n",
    "        doc_id=int(doc_id)\n",
    "        if num ==100:\n",
    "            break\n",
    "        text = decode(text)\n",
    "        terms = list()\n",
    "        for word in tokenizer(text):\n",
    "#           romove the punctuation and stopwords which could show the meaningful words in the query \n",
    "            if word not in punctuation and word not in stopwords_ger:\n",
    "                terms.append(word)\n",
    "        doc_list[doc_id] = terms\n",
    "    return doc_list\n",
    "\n",
    "query_list=preprocess(filename)\n",
    "output_list = sorted(query_list.items(), key=lambda d:d[0])\n",
    "print output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "c439f8fa528b6886a6cd8043813145dce5705332843ae927143e991b"
   },
   "outputs": [],
   "source": [
    "# taking the maximum probability of translation into English p(e|f)\n",
    "\n",
    "stopwords_eng = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def find_largest_prob(word):\n",
    "    return max(m_GertoEng.translation_table[word].iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "def translate_one_query(sentence):\n",
    "    eng_sentence=\"\"\n",
    "    for word in sentence:\n",
    "#         print m_GertoEng.translation_table[word]\n",
    "        if word in m_GertoEng.translation_table:\n",
    "            eng_word=str(find_largest_prob(word))\n",
    "            if eng_word==\"None\":\n",
    "                eng_word=\"\"\n",
    "        else:\n",
    "            eng_word= \"\"\n",
    "        if eng_word==\"\":\n",
    "            eng_sentence=eng_sentence\n",
    "        else:\n",
    "            eng_sentence=eng_sentence+eng_word+\" \"\n",
    "    return eng_sentence\n",
    "\n",
    "def translate_query_list(query_dic):\n",
    "    result_dic={}\n",
    "    for key in query_list:\n",
    "        result_dic[key]=translate_one_query(query_list[key])\n",
    "    output_list = sorted(result_dic.items(), key=lambda d:d[0])\n",
    "    for key in output_list:\n",
    "        print \"Queryid: \"+str(key[0])\n",
    "        print \"Translation Result: \"+key[1]    \n",
    "    return output_list\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "result_maxprob=translate_query_list(output_list)\n",
    "# write the first 100 queries into the disc\n",
    "filepath = \"data_files_ass2/translation_maxprob.txt\"\n",
    "f1 = open(filepath,\"w\")\n",
    "for i in range(len(result_maxprob)):\n",
    "    f1.write(str(result_maxprob[i][0])+'\\t'+result_maxprob[i][1]+'\\n')\n",
    "f1.close()\n",
    "\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "    \n",
    "# translate_one_query(query_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the noisy-channel probability, p(f|e)p(e)\n",
    "# p(e):\n",
    "#   Unigram model of the bi_text_large.en is used to calculate the laguagemodel score\n",
    "# p(f|e):\n",
    "#   Ibm1 model trained before From English to German used in calculating the Translation model score \n",
    "import operator\n",
    "\n",
    "unigrams, bigrams,trigrams, token_count = get_counts(training_set)\n",
    "w_lm=1\n",
    "w_tl=1\n",
    "\n",
    "# language model which rewards a better english word\n",
    "def unigramScore(query,unigrams,token_count):\n",
    "    if query in stopwords_eng:\n",
    "#       down scaling the stopwords as the stopwords have much higher unigram probablility\n",
    "        score= float(unigrams[query])/(float(token_count)*10000)\n",
    "    else:\n",
    "        score= float(unigrams[query])/float(token_count)\n",
    "    return score\n",
    "\n",
    "# try to find the key in translation_table that form a dict of Eng words could possiblly translated to query\n",
    "def find_key(query,translation_table):\n",
    "    potential_dic={}\n",
    "    for key,values in translation_table.items():\n",
    "        if query in values:\n",
    "            if translation_table[key][query]>0.001 :\n",
    "                potential_dic[key]=translation_table[key][query]\n",
    "    return potential_dic\n",
    "\n",
    "# find the best matched english words with highest results combining language model and translation model\n",
    "def best_match(query,translation_table,unigrams,token_count):\n",
    "    results = {}\n",
    "    result=\"\"\n",
    "    potential_dic = find_key(query,translation_table)\n",
    "#     print result_dic\n",
    "    if potential_dic =={}:\n",
    "        return \"\"\n",
    "    else:\n",
    "        for word,value in potential_dic.items():\n",
    "            lm_score = unigramScore(word,unigrams,token_count)\n",
    "            tl_score = value\n",
    "            score = (lm_score)*w_lm*(tl_score)*w_tl\n",
    "            results[word] = score\n",
    "    result= max(results.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    return result\n",
    "\n",
    "# find the best match english words in the sentence one by one\n",
    "def translate_sentence(sentence):\n",
    "    eng_sentence=\"\"\n",
    "    for word in sentence:\n",
    "        eng_word=best_match(word,m_EngtoGer.translation_table,unigrams,token_count)\n",
    "        if eng_word==\"\":\n",
    "            continue\n",
    "        else:\n",
    "            eng_sentence=eng_sentence+eng_word+\" \"\n",
    "    return eng_sentence\n",
    "\n",
    "def translate_query(query_list):\n",
    "    result_dic={}\n",
    "    for key in query_list:\n",
    "        result_dic[key]=translate_sentence(query_list[key])\n",
    "    output_list = sorted(result_dic.items(), key=lambda d:d[0])\n",
    "    for key in output_list:\n",
    "        print \"Queryid: \"+str(key[0])\n",
    "#     t=re.sub(\"UNK\",'',t)\n",
    "        print \"Translation Result: \"+key[1]\n",
    "    return output_list\n",
    "    \n",
    "start =time.clock()\n",
    "\n",
    "# print best_match('haus',m_EngtoGer.translation_table,unigrams,token_count)\n",
    "result_noisechannel=translate_query(query_list)\n",
    "\n",
    "filepath = \"data_files_ass2/translation_unigram_noisychannel.txt\"\n",
    "f2 = open(filepath,\"w\")\n",
    "for i in range(len(result_noisechannel)):\n",
    "    f2.write(str(result_noisechannel[i][0])+'\\t'+result_noisechannel[i][1]+'\\n')\n",
    "f2.close()\n",
    "end = time.clock()\n",
    "\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "98984860354538a8b410d3d87ad7cb089d3c5ecd7506e56fe8ffb498"
   },
   "source": [
    "Now you will need to write some text about the procedure above, and how well it worked. You should answer:\n",
    "  - how good are the translations? How do your methods fare at translating the content words versus functions words? You can use Google translate to provide an alternative (pretty good) translation for comparison purposes. \n",
    "  - what are good settings for various modelling parameters such as language model discounting, translation model smoothing, any decoding parameters, and how do these affect the outputs?\n",
    "  - compare the language model perplexity for the unigram and trigram models over your decoder outputs, how does the difference in perplexity between the two models compare to your test above on the monolingual data? \n",
    "  - how do the learned alignments differ between the two translation directions, and does their combination appear oto improve the predictions? (You don't need a formal evaluation here, as we have no *gold standard* alignments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "e0a8f5b2957eed468bab0378020abb456b40b87f53207e261e762359"
   },
   "source": [
    "--Most of the result of translation is pretty bad, as the context information are all missing no matter for max probability or noise channel method. It could be seen that most of the content word could be well translated comparing the function words. Some of the stopwords like \"der\" is translated into \"detering\" which doesn't make sense at all.\n",
    "\n",
    "--In particular the decoding parameter is pretty important in this case, as the corpu is pretty big that most of the unigram model result in very small number but the stopwords which highly shown in the corpus have a very high prob. In order to have a reasonable result a higher parameter given to the LM and a penalty given to the stopwords lead to a better result.\n",
    "\n",
    "-- The model with lower perplexity seems more reliable than the small one, especially for the case of comparing trigram modle and unigram model.\n",
    "\n",
    "--The alignment on both side may give a better understanding because the alignment of the words shows some different characters in these two language. The model gradually learned these patterns which could benefit the improvement of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "fca99a32425faa411854dfdd1d43e55855c0073e658d0e6481a41355"
   },
   "source": [
    "## Putting the CLIR system together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "cae07298afecf34ed54b92ef762244d951904b44a77192f161082678"
   },
   "source": [
    "Now you should couple the translation and IR components from above. Take your translated queries and use these with the IR system to find the most relevant documents. You should then evaluate these predictions against the supplied relevance judgements, using the *mean average precision* (MAP) metric. As part of this, you will want to consider tuning the settings of the IR system for best performance, and comment on how successful your approach was and evaluate the IR performance is affected by the parameter settings and modelling decisions you have made.\n",
    "\n",
    "Note that if you were stuck on the translation steps above, or your translations are otherwise unusable, you can implement this step using google translate outputs for the 100 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "00cde97386e4362053e20e8399b4ad377eb6bcdd6e0c402c3f6ef71b"
   },
   "outputs": [],
   "source": [
    "# output_list could be chosed from the translation results from the max translation \n",
    "# probability or max unigram noisy-channel probability\n",
    "\n",
    "def preprocess_query(doc):\n",
    "    terms = list()\n",
    "    doc= decode(doc)\n",
    "    for token in doc.split():\n",
    "        if token not in punctuation and token not in stopwords_eng: \n",
    "            terms.append(stemmer.stem(token))\n",
    "    text=\" \".join(terms)\n",
    "    return text\n",
    "\n",
    "def get_query(query_list):\n",
    "    q_list={}\n",
    "    k=0\n",
    "    for query in query_list:\n",
    "        if k<10:\n",
    "            text=preprocess_query(query[1])\n",
    "            q_list[query[0]]=text\n",
    "            k+=1\n",
    "    return q_list\n",
    "\n",
    "query=get_query(result_maxprob)\n",
    "# query=get_query(result_noisechannel)\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_actual_list(filename):\n",
    "    path= relative_path(filename)\n",
    "    dic={}\n",
    "    docs=[]\n",
    "    f = open(path,'r')\n",
    "    for line in f:\n",
    "        docs.append(line.split('\\t')[2])\n",
    "        dic[line.split('\\t')[0]]=docs\n",
    "    return dic\n",
    "\n",
    "\n",
    "def get_map_score(queryId,resultList,relDic,k):\n",
    "    resultList.reverse()\n",
    "    results=resultList[0:k] #[(docid,score),...]\n",
    "    i = 1\n",
    "    precision = 0.0\n",
    "    count=0.0\n",
    "    while i < k:\n",
    "        if results[i][0] in relDic[queryId]:\n",
    "            count+=1\n",
    "            precision += count/i\n",
    "        i += 1\n",
    "    p_ave=precision/count\n",
    "    return p_ave\n",
    "\n",
    "k=10\n",
    "filename= \"dev.qrel\"\n",
    "map_dict={}\n",
    "total_map=0\n",
    "predicted_list={}\n",
    "\n",
    "actual_list = get_actual_list(filename)\n",
    "\n",
    "for queryid,text in query.items():\n",
    "    print queryid\n",
    "    predicted_list[queryid]= query_One(text,doc_list,inverted_index)\n",
    "    print predicted_list[queryid]\n",
    "    total_map+=get_map_score(queryid,predicted_list[queryid],actual_list[queryid],k)\n",
    "print total_map/float(len(query))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "209bcf439078d936e68e1432c0c1f2b018a624c18feb98800a53fcfb"
   },
   "source": [
    "The mean average precision is used to evaluate the result of translation model within the entire docs and produce the most relevant 100 results for calculation. The result is pretty bad as the unigram noise channel doesn't perform well and either the max probablity model.It could be the problem of the word alignment is not that good in this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "962d8abff77d2a0602a2bba88a8b4fbc1efc7bb8b47e2be197b9e7ff"
   },
   "source": [
    "Note that in a complete CLIR you would translate the retreived documents back into German. We won't bother about that for this assignment, especially as I doubt many of you can understand German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2b12eccfb5b581afddc3573ca82566c257e47e20da1598cf7e5c2fee"
   },
   "source": [
    "# Part 2: Extension and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0625c530df1178cd70463194a87bf20466fea6842123d52720a6ecdc"
   },
   "source": [
    "## Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f49134c38f52c82c2fb1af0fc1732ce950bfab4f217af78c9411e75e"
   },
   "source": [
    "You now have a working end-to-end CLIR system. The next step is to revisit some of the steps above to see if you can improve the performance of the system. You are invited to be creative with your ideas about how to do this. Note that many great ideas might not produce improvements, or end up being very difficult to implement. This will not be assessed solely on the grounds of CLIR accuracy; we will be looking primarily for interesting and creative ideas.\n",
    "\n",
    "Here are some ideas on what you might do. Warning: some are bigger than others, aim to spend 5-6 hours in total on this.\n",
    "\n",
    "#### Structured decoding\n",
    "\n",
    "Decode using a higher order language model, possibly with word reordering. Using your trigram language model and a word-based or phrase-based translation model, search for the best scoring translation sentence. You may want to use the tools in *nltk.translate*, including the phrase extraction and *StackDecoder*, especially if you wish to support word reordering. The decoding algorithm is much simpler (and tractable) without reordering, so you might want to implement this yourself. You will want to work with log-probabilities to avoid numerical underflow. Note  that the *StackDecoder* is not highly stable code, so you will need to understand its code in detail if you chose to use it. You will need to supply translation log probabilities and language model log probabilities as input to the decoder and consider the context of the previous two words in estimating the probability of each word. \n",
    "\n",
    "#### Probabilisitc querying\n",
    "\n",
    "Consider queries including weighted terms based on multiple translation outputs, such as the word translation probabilities or using *k-best* translations from a decoder (you will need to extract not just the best translation from the decode, but the runners up, which can be done from the stack contents after beam search.)  You should consider how best to define TF and DF for query words based on their ambiguous translations. You may find inspiration in the bibliography of [Douglas Oard](https://terpconnect.umd.edu/~oard/research.html#mlia) such as [this paper](https://terpconnect.umd.edu/~oard/pdf/coling12ture.pdf).\n",
    "\n",
    "#### Word alignment\n",
    "\n",
    "Implement an extended word alignment model, such as the *hmm* model (see [Vogel's paper](http://www.aclweb.org/anthology/C96-2141)) or a variant of *ibm2*. The variant of *ibm2* could include a better formulation for the distortion term, $p(i|j,I,J)$, based around rewarding values of i and j that are at similar relative positions to encourage alignments near the diagonal. This contrasts with *ibm2* which just learns a massive table of counts for all combinations of i,j,I and J. This strictly needs normalisation, as its not a probability, but for simplicity you can ignore this aspect here. This idea is inspired by [fast-align](https://github.com/clab/fast_align) which has a similar, but slightly more complex, formulation.\n",
    "\n",
    "You might also want to experiment with [fast-align](https://github.com/clab/fast_align) (an extremely fast and accurate variant of *ibm2*) or [GIZA++](http://www.statmt.org/moses/giza/GIZA++.html) (an implementation of *ibm1* - *ibm5* and the *hmm*), both widely used and highly optimised alignment tools. Neither of these tools are in python, and may be a little involved to install and compile.\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "Experiment with a state-of-the-art translation tool like [Moses](http://www.statmt.org/moses/).  Can you get better CLIR performance using this, trained on the full parallel dataset? You might want to consider translating the document collection into German, and then doing the IR entirely in German. You might consider building a python interface to moses, building on the existing very basic funcationality. **Warning:** Moses is a complex suite of software, and can be a little complex to install and use. However there are good installation and usage guides on the moses webpage.\n",
    "\n",
    "#### IR engine\n",
    "\n",
    "There are several great open source IR engines, including [Lucene](https://lucene.apache.org/core/), [Terrier](http://terrier.org/), [Zettair](http://www.seg.rmit.edu.au/zettair/) and [Lemur](http://www.lemurproject.org/). You could experiment with these, and use this to compare the accuracy of different types of IR models on the CLIR data. Alternatively, you could implement a faster or more compact retreival system in python, e.g., using compressed vbyte encodings, skip lists or similar.\n",
    "\n",
    "Note that the development of some of the extensions maye require working in the command shell and in other languages than python. If this is the case, you should attach your code in separate files as part of your submission, but include the analysis text here. *Note that excellent submissions implementing interfaces or models not currently in NLTK will be considered for inclusion into the toolkit.* You should include both the code, and text explaining your work and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "c4a684a7cd8c2c487e182fa614b5218b876a43166bd4dd9ac9ece258"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate import *\n",
    "from math import log\n",
    "\n",
    "start =time.clock()\n",
    "\n",
    "bitext_EngtoGer = get_bitext(text_e,text_f)\n",
    "\n",
    "bt_EngtoGer=[AlignedSent(E,F) for E,F in bitext_EngtoGer]\n",
    "\n",
    "m=IBMModel1(bt_EngtoGer, 10)\n",
    "\n",
    "end=time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the \"None\" in alignment\n",
    "align=[]\n",
    "for i in range(len(bt_EngtoGer)):\n",
    "    a=bt_EngtoGer[i].alignment\n",
    "    align.append(str(a))\n",
    "    if None in align:\n",
    "        del align[None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.translate.api import PhraseTableEntry\n",
    "class PhraseTable(object):\n",
    "    def __init__(self):\n",
    "        self.src_phrases = dict()\n",
    "        \n",
    "    def translations_for(self, src_phrase):\n",
    "        if len(src_phrase)==1:\n",
    "            if src_phrase not in self.src_phrases or self.src_phrases[src_phrase] == [] :\n",
    "                return [PhraseTableEntry(trg_phrase=src_phrase, log_prob=0.0)]\n",
    "        return self.src_phrases[src_phrase]\n",
    "    \n",
    "    def add(self, src_phrase, trg_phrase, log_prob):\n",
    "        entry = PhraseTableEntry(trg_phrase=trg_phrase, log_prob=log_prob)\n",
    "        if src_phrase not in self.src_phrases:\n",
    "            self.src_phrases[src_phrase] = []\n",
    "        self.src_phrases[src_phrase].append(entry)\n",
    "        self.src_phrases[src_phrase].sort(key=lambda e: e.log_prob,\n",
    "                                          reverse=True)\n",
    "    def __contains__(self, src_phrase):\n",
    "        if len(src_phrase)==1:\n",
    "            return True\n",
    "        return src_phrase in self.src_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trigrams LM model\n",
    "language_prob=get_log_prob_interp(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count, trigram_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# phrase extraction\n",
    "from nltk.translate.phrase_based import phrase_extraction\n",
    "\n",
    "phrase_pair_counts = defaultdict(Counter)\n",
    "for s in bt_EngtoGer:\n",
    "    if None not in s.alignment:\n",
    "        phrases = phrase_extraction(' '.join(s.words), ' '.join(s.mots), s.alignment, max_phrase_length=3)\n",
    "        for en_span, fr_span, en_phrase, fr_phrase in phrases:\n",
    "            phrase_pair_counts[tuple(en_phrase.split())][tuple(fr_phrase.split())] += 1\n",
    "\n",
    "phrase_table = PhraseTable()\n",
    "for en_phrase, fr_counts in phrase_pair_counts.items():\n",
    "    total = sum(fr_counts.values())\n",
    "    for fr_phrase, count in fr_counts.items():\n",
    "        phrase_table.add(fr_phrase, en_phrase, log(count / float(total)))\n",
    "        \n",
    "class LM:\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs        \n",
    "    def probability_change(self, context, phrase): \n",
    "        # Used when expanding a hypothesis with a new phrase\n",
    "        # (higher order LMs would need to look at the word sequence, including context)\n",
    "        return sum([self.probs[word] for word in phrase])\n",
    "    def probability(self, phrase): \n",
    "        # Used for future cost estimation only, to get a cheap (approximate) LM score for each phrase \n",
    "        return sum([self.probs[word] for word in phrase])\n",
    "language_model = LM(language_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1de0b5e4b733d2bb5557d865759e9caa1e2f857cc50b3f306b21e389"
   },
   "source": [
    "Try to use the stack decoder to build a noise channel model with phrase alignments and trigrams language model for better performance, but stuck in the phrase table forming, the the result of alignments in ibm model contains \"None\" which is not able to deal with in the phrase. Even I tried to change the code in nltk PhraseTable, still could not deal with \"None\". Tried to delete the \"None\" in alignment as suggested in the discussion, but the alaignment type not allowed for del[] function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "22a46e982afb944df0a642ad2a948799a2bc25b34f06c95aff1f6f79"
   },
   "source": [
    "## Discussion \n",
    "\n",
    "What conclusions can you make about CLIR, or more generally translation and IR? Overall what approaches worked and what ones didn't? What insights do you have from doing this, put another way, if you were to start again, what approach would you take and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "8b9ab115f429e6e2b870ec187180aede985ee333b03bfe98efe7ff94"
   },
   "source": [
    "About the CLIR, I think the combination of the translation and information retrival could generate the better. It is a good method to using noise channel algorithm which incorperate the idea of both translation result and useful information from language model.In a lot of circumstance, only using the probability of translation model results into a meaningless translation as the highest probability is rigid. It could be easily find out that in most of the documents, the phrase based alignment could have far more better performance than word alignment. It is pretty obvious as the phrase alignment has better information in the order of the words which is very crucial in this point. \n",
    "\n",
    "Additionaly, the translation model like IBM 3 invovle reordering of the resutls alignment which is a much better way to get useful result. But it seems the IBM 3 model trainging process is really time-consuming or it requires more powerful computer to train it because I tried the training of the model and based on the bitext_small given, it couldn't generate the result alignments in 30mins.\n",
    "\n",
    "Overall the idea of precomputing the data in doc before querying is very helpful, at the start, I didn't store any value in when process the documents, it ended up really slow that take 3 mins to compute one query. After storing everythin i can in the preprocess step, it becomes much quicker then before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
